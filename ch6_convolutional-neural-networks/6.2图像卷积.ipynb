{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f7d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5064536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]),\n",
       " torch.Size([2, 2]),\n",
       " 3,\n",
       " 3,\n",
       " tensor([[19., 25.],\n",
       "         [37., 43.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。\n",
    "# 在corr2d函数中实现如上过程，该函数接受输入张量X和卷积核张量K，并返回输出张量Y。\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # h=2 w=2\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 3-2+1.  3-2+1\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "#             print(X[i:i + h, j:j + w])\n",
    "#             print(X[i:i + h, j:j + w] * K)\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "X.shape, K.shape, X.shape[0],X.shape[1], corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62e5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积层\n",
    "# 定义的corr2d函数实现二维卷积层。在__init__构造函数中，将weight和bias声明为两个模型参数。\n",
    "# 前向传播函数调用corr2d函数并添加偏置。\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight=nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias=nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return corr2d(x,self.weight)+self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae92f23",
   "metadata": {},
   "source": [
    "- 无论是左边还是右边逗号都要靠近冒号：\n",
    "- 如果冒号：的左边或者右边还有冒号，这时候就说明其中一个冒号代表的是范围（eg:1:5 从１到４）\n",
    "- 如果冒号：左边或者右边没有任何东西，那么这时候代表全体\n",
    "- [a:b] 对ａ的改变是行的改变，对ｂ的改变是队列的改变\n",
    "- 负数在左侧，则从后往前数n个\n",
    "- 负数在右侧，则是排除了后n个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57eb244d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15],\n",
       "        [16, 17],\n",
       "        [18, 19]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取法\n",
    "x=torch.tensor([[0,1],[2,3],[4,5],[6,7],[8,9],[10,11],[12,13],[14,15],[16,17],[18,19]])  \n",
    "x\n",
    "\n",
    "# x[:,0] #　二维数组取第１维所有数据\n",
    "# x[:,1] # 第２列\n",
    "# x[0,:] # 第１行\n",
    "# x[3,:] # 第4行\n",
    "# x[1:4,:] # 第一二三行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c685993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.ones(6,8)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529516b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,2:6]=0 # 第一维全取（行） 第二维取2到6列\n",
    "X\n",
    "# 中间四列为黑色（0），其余像素为白色（1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9935bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1., -1.]]),\n",
       " tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造一个高度为1、宽度为2的卷积核K\n",
    "K=torch.tensor([[1.0,-1.0]])\n",
    "Y=corr2d(X,K)\n",
    "K , Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2405c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 \n",
    "# 这个卷积核K只可以检测垂直边缘，无法检测水平边缘。\n",
    "\n",
    "corr2d(X.t(),K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f834527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,loss26.157\n",
      "epoch:1,loss14.287\n",
      "epoch:2,loss8.139\n",
      "epoch:3,loss4.797\n",
      "epoch:4,loss2.902\n",
      "epoch:5,loss1.788\n",
      "epoch:6,loss1.116\n",
      "epoch:7,loss0.703\n",
      "epoch:8,loss0.445\n",
      "epoch:9,loss0.283\n"
     ]
    }
   ],
   "source": [
    "conv2d=nn.Conv2d(1,1,kernel_size=(1,2),bias=False)\n",
    "\n",
    "X=X.reshape((1,1,6,8))\n",
    "Y=Y.reshape((1,1,6,7))\n",
    "\n",
    "lr=3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    y_hat=conv2d(X)\n",
    "    loss=(y_hat-Y)**2\n",
    "    conv2d.zero_grad()\n",
    "    loss.sum().backward()\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:]-=lr*conv2d.weight.grad\n",
    "    \n",
    "    print(f'epoch:{i},loss{loss.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528b8cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9346, -1.0437]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4bfd5",
   "metadata": {},
   "source": [
    "- 二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。\n",
    "\n",
    "- 我们可以设计一个卷积核来检测图像的边缘。\n",
    "\n",
    "- 我们可以从数据中学习卷积核的参数。\n",
    "\n",
    "- 学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。\n",
    "\n",
    "- 当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8522d0",
   "metadata": {},
   "source": [
    "#### 6.3填充和步幅\n",
    "- 填充：由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法即为填充（padding）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfdc6b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为了方便起见，我们定义了一个计算卷积层的函数。\n",
    "# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # 这里的（1，1）表示批量大小和通道数都是1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # 省略前两个维度：批量大小和通道\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424af79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。\n",
    "# 使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1。\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1)) # 1,1 是in 和 out 的通道数 RGB为3\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59390ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 步幅 为2\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c5d1daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a80f1",
   "metadata": {},
   "source": [
    "#### 6.4多输入多输出通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9e6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "    # 先遍历‘X’和‘K’的第0个维度（通道维度）再把他们加起来\n",
    "    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))  # zip(iterables) 创建一个迭代器，它从每个迭代器中聚合元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30267997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e6451b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in_out(X,K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],0)\n",
    "\n",
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c3d9b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda258f",
   "metadata": {},
   "source": [
    "- 多输入多输出通道可以用来扩展卷积层的模型。\n",
    "\n",
    "- 当以每像素为基础应用时，1x1卷积层相当于全连接层。\n",
    "\n",
    "- 1x1卷积层通常用于调整网络层的通道数量和控制模型复杂性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b6c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90803524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
