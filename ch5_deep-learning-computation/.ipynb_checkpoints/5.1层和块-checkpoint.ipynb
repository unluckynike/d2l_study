{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71b7c8f",
   "metadata": {},
   "source": [
    "#### 单个神经网络 （1）接受一些输入； （2）生成相应的标量输出； （3）具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228bd371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " tensor([[0.0953, 0.6667, 0.8204, 0.6101, 0.1762, 0.4341, 0.7488, 0.6280, 0.3098,\n",
      "         0.0705, 0.1566, 0.5482, 0.2047, 0.1799, 0.4280, 0.6639, 0.8038, 0.9063,\n",
      "         0.1340, 0.0692],\n",
      "        [0.5574, 0.9293, 0.3582, 0.7216, 0.3751, 0.5074, 0.3384, 0.9024, 0.0916,\n",
      "         0.1738, 0.5816, 0.1524, 0.2381, 0.6329, 0.3661, 0.0359, 0.4315, 0.0630,\n",
      "         0.3457, 0.5530]])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0238,  0.1644, -0.0494, -0.2460, -0.0078,  0.1611, -0.2197,  0.1671,\n",
       "         -0.0697,  0.1329],\n",
       "        [ 0.0751,  0.2071, -0.0103, -0.2214, -0.0429,  0.1264, -0.1132,  0.2032,\n",
       "          0.0999, -0.0709]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层， \n",
    "# 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。\n",
    "net = nn.Sequential(nn.Linear(20,256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(256,10))\n",
    "\n",
    "# nn.Sequential定义了一种特殊的Module， 即在PyTorch中表示一个块的类， \n",
    "# 它维护了一个由Module组成的有序列表\n",
    "X=torch.rand(2,20)\n",
    "print('X:\\n',X)\n",
    "print(net)\n",
    "net(X)\n",
    "net.__call__(X) # 等价于net（X） 它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937712e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1110, -0.1417, -0.1161,  0.1328, -0.0016,  0.0264, -0.2174, -0.2400,\n",
       "          0.0388,  0.0599],\n",
       "        [-0.0816, -0.1997,  0.0352,  0.0932, -0.0291, -0.0831, -0.0345, -0.0403,\n",
       "         -0.0572,  0.0630]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义块\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.relu=nn.ReLU()\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "net=MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ddb8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2492,  0.0092,  0.0392, -0.0341, -0.1914, -0.0339, -0.0016, -0.1108,\n",
       "          0.1734,  0.0344],\n",
       "        [ 0.2744,  0.1119, -0.0821, -0.0062, -0.1286,  0.0606, -0.0576, -0.0497,\n",
       "          0.0824, -0.0460]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顺序块\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707af8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedHiddenMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1118, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 \n",
    "# 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "    \n",
    "net = FixedHiddenMLP()\n",
    "print(net)\n",
    "net(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5a00762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (linear): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=16, out_features=20, bias=True)\n",
      "  (2): FixedHiddenMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0298, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#混合\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), \n",
    "                        nn.Linear(16, 20), \n",
    "                        FixedHiddenMLP())\n",
    "print(chimera)\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15bd4c",
   "metadata": {},
   "source": [
    "- 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "\n",
    "- 块可以包含代码。\n",
    "\n",
    "- 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "\n",
    "- 层和块的顺序连接由Sequential块处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f9f42",
   "metadata": {},
   "source": [
    "#### 5.2参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65930bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " tensor([[0.4982, 0.4627, 0.4973, 0.3981],\n",
      "        [0.7209, 0.7760, 0.4737, 0.1656]])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1725],\n",
       "        [0.1149]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单层隐藏的MLP\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(8,1)\n",
    "                   )\n",
    "X=torch.rand(2,4)\n",
    "print('X:\\n',X)\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54a9944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0020,  0.1918, -0.3216,  0.0881, -0.1480, -0.1401,  0.1077,  0.2406]])), ('bias', tensor([0.0588]))])\n"
     ]
    }
   ],
   "source": [
    "# 参数访问\n",
    "# 通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "# 这个全连接层包含两个参数，分别是该层的权重和偏置。 两者都存储为单精度浮点数（float32）。\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "178df9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0588], requires_grad=True)\n",
      "tensor([0.0588])\n",
      "Parameter containing:\n",
      "tensor([[-0.0020,  0.1918, -0.3216,  0.0881, -0.1480, -0.1401,  0.1077,  0.2406]],\n",
      "       requires_grad=True)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 目标参数\n",
    "\n",
    "print(type(net[2].bias))\n",
    "\n",
    "print(net[2].bias)\n",
    "\n",
    "print(net[2].bias.data)\n",
    "\n",
    "print(net[2].weight)\n",
    "\n",
    "print(net[2].weight.grad)\n",
    "\n",
    "# 参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 \n",
    "# 还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态\n",
    "net[2].weight.grad==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4115fbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0588])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一次性访问所有 参数\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])\n",
    "\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3f30e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1482,  0.2590, -0.4488,  0.3266],\n",
       "        [-0.2021, -0.3671,  0.3030, -0.2204],\n",
       "        [-0.4761, -0.1170, -0.2478, -0.0526],\n",
       "        [ 0.3807, -0.0631, -0.3186, -0.1683],\n",
       "        [ 0.0377,  0.0895,  0.1627, -0.4664],\n",
       "        [-0.3386,  0.0997,  0.0383,  0.0738],\n",
       "        [-0.3315,  0.0724,  0.4479,  0.3165],\n",
       "        [-0.4685, -0.2520,  0.4976,  0.4568]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.weight'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a016b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 嵌套块\n",
    "\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(8,4),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "\n",
    "def block2():\n",
    "    net =nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 嵌套四层\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net\n",
    "\n",
    "mnet=nn.Sequential(block2(),  # 一个四层嵌套的一号块\n",
    "                  nn.Linear(4,1)  #二号块 仅一层\n",
    "                  )\n",
    "mnet(X)\n",
    "print(mnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adcbe215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1251, -0.1369, -0.0213,  0.4186,  0.0520, -0.3110,  0.4742,  0.4635],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问第一个主要的块中、第二个子块的第一层的偏置项。\n",
    "mnet[0][1][0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c96e3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数初始化  内置初始化\n",
    "# 将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "print(net)\n",
    "net[0].bias.data # 偏置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7ed28f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1750e-03, -1.9324e-02, -9.3516e-03,  9.2029e-03],\n",
       "        [ 3.8475e-03, -1.6744e-03,  1.4794e-03, -2.6204e-03],\n",
       "        [-1.5931e-02, -1.6734e-03,  1.1294e-02, -8.4666e-03],\n",
       "        [-9.0231e-04, -9.0943e-03, -8.0805e-03, -5.0911e-03],\n",
       "        [-1.0957e-05, -5.1908e-03, -2.0363e-03, -1.4145e-02],\n",
       "        [-1.9736e-02, -2.5984e-03, -5.2965e-03,  7.7533e-03],\n",
       "        [-8.7172e-03, -1.2843e-02,  4.8050e-03,  1.2975e-04],\n",
       "        [ 9.7099e-03, -2.3270e-03, -5.8970e-03,  1.6640e-03]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f9d9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以将所有参数初始化为给定的常数，比如初始化为1。\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "print(net)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54409e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=1, bias=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。\n",
    "def init_xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "           \n",
    "print(net)    \n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df4a9ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6790,  0.1048, -0.0592,  0.4223],\n",
       "        [ 0.1797, -0.0943,  0.0594, -0.6053],\n",
       "        [ 0.5692, -0.0439, -0.4474, -0.6623],\n",
       "        [-0.2262,  0.1571,  0.0949,  0.2654],\n",
       "        [-0.4595,  0.2230,  0.4786, -0.4682],\n",
       "        [ 0.5714,  0.4970, -0.3600,  0.2391],\n",
       "        [ 0.2287, -0.1324,  0.5787, -0.3958],\n",
       "        [-0.2618, -0.3813, -0.6119, -0.5108]], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07be269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]], requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdb51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c58e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1c71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dad959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee2961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba419628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7b511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce4ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdc45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f471f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
